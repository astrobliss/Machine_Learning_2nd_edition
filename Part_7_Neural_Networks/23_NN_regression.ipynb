{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Regression\n",
    "## Boston housing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      crim    zn  indus  chas    nox     rm   age     dis  rad  tax  ptratio  \\\n",
      "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296     15.3   \n",
      "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242     17.8   \n",
      "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242     17.8   \n",
      "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222     18.7   \n",
      "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222     18.7   \n",
      "\n",
      "    black  lstat  medv  \n",
      "0  396.90   4.98  24.0  \n",
      "1  396.90   9.14  21.6  \n",
      "2  392.83   4.03  34.7  \n",
      "3  394.63   2.94  33.4  \n",
      "4  396.90   5.33  36.2  \n",
      "\n",
      "Dimensions of data frame: (506, 14)\n"
     ]
    }
   ],
   "source": [
    "### load the data\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/Boston.csv')\n",
    "print(df.head())\n",
    "print('\\nDimensions of data frame:', df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: (404, 12)\n",
      "test size: (102, 12)\n"
     ]
    }
   ],
   "source": [
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.iloc[:, 0:12]\n",
    "y = df.iloc[:, 13]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
    "\n",
    "print('train size:', X_train.shape)\n",
    "print('test size:', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression as a baseline\n",
    "\n",
    "This code is copied from the 22_Linear_Regression notebook in the GitHub, so refer there for further information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse= 27.442176730255614\n",
      "correlation= 0.7326596279331063\n"
     ]
    }
   ],
   "source": [
    "## train the algorithm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "y_pred = linreg.predict(X_test)\n",
    "\n",
    "# evaluation\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "print('mse=', mean_squared_error(y_test, y_pred))\n",
    "print('correlation=', r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling the data\n",
    "\n",
    "A neural network will convert faster and get better results with scaled data. The following two code blocks show two different ways to scale the data. Using either scaling method is fine but it is important to scale using means and standard deviations from the train set only so that a firewall is maintained between the train and test sets. \n",
    "\n",
    "\n",
    "The first block shows how to scale manually using built-in Python and pandas functionality. Normalizing data consists of two transformations: first the mean is subtracted from every element, then each element is divided by the standard deviation. The primary reason for showing this code is to reinforce the concept of normalization. Normalization changes the shape of data to a normal distribution whereas scaling just scales the data to fit a certain numeric range. \n",
    "\n",
    "The second block shows how to scale using functionality from sklearn. Note that the sklearn standard scaler used in the second block will normalize the data unless with with_mean and with_std args are set =False.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data using Python and pandas functionality\n",
    "\n",
    "mean = X_train.mean(axis=0)\n",
    "X_train -= mean\n",
    "std = X_train.std(axis=0)\n",
    "X_train /= std\n",
    "\n",
    "X_test -= mean\n",
    "X_test /= std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data using sklearn functionality\n",
    "from sklearn import preprocessing\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the algorithm\n",
    "\n",
    "There are many options available to set up the multi-layer perceptron (MLP) regressor. See [the docs](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor)\n",
    "\n",
    "Running the regressor with default options on the first try resulted in the error message that it failed to converge. Specifying the layers size, and more importantly increasing the maximum number of iterations allowed it to converge. The default settings can be seen on the link above.\n",
    "\n",
    "A random state is used so that the code gets the same results every time it is run. The randomness comes into play because weights are initially set to random values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "             beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "             hidden_layer_sizes=(6, 3), learning_rate='constant',\n",
       "             learning_rate_init=0.001, max_fun=15000, max_iter=500,\n",
       "             momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "             power_t=0.5, random_state=1234, shuffle=True, solver='adam',\n",
       "             tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "             warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the algorithm\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "regr = MLPRegressor(hidden_layer_sizes=(6, 3), max_iter=500, random_state=1234)\n",
    "regr.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "\n",
    "y_pred = regr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse= 29.309654210171164\n",
      "correlation= 0.7144667517187085\n"
     ]
    }
   ],
   "source": [
    "# evaluation\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "print('mse=', mean_squared_error(y_test, y_pred))\n",
    "print('correlation=', r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try different settings\n",
    "\n",
    "The default solver is adam, as you can see in the output from the training above. The documentation recommends lbfgs for smaller data sets, stating that it will converge faster and get better results. With the 500 iteration and the lbfgs solver, it did not converge, so the max iterations was bumped up to 1500. As you can see it did get better results than the previous network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "             beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "             hidden_layer_sizes=(6, 3), learning_rate='constant',\n",
       "             learning_rate_init=0.001, max_fun=15000, max_iter=1500,\n",
       "             momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "             power_t=0.5, random_state=1234, shuffle=True, solver='lbfgs',\n",
       "             tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "             warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr = MLPRegressor(hidden_layer_sizes=(6, 3), solver='lbfgs', max_iter=1500, random_state=1234)\n",
    "regr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse= 11.558679390825924\n",
      "correlation= 0.8873958986810827\n"
     ]
    }
   ],
   "source": [
    "y_pred = regr.predict(X_test)\n",
    "\n",
    "print('mse=', mean_squared_error(y_test, y_pred))\n",
    "print('correlation=', r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "Linear regression achieved mse=27.4 which is better than the first neural network's mse=29.3. The second neural network outperformed them both with an mse=11.6. Using the lbfgs solver and a higher number of iterations was able to achieve better results.\n",
    "\n",
    "Although several other network sizes besides (6, 3) were tested, more complex models performed worse. Since this is a small data set, a complex network will overfit and learn noise from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
